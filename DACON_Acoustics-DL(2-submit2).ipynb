{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm","authorship_tag":"ABX9TyN/ZA6FTN43OC3Z1yKoIhWj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","path = '/content/gdrive/My Drive/Colab Notebooks/DACON/ACOUSTIC/'\n","\n","import os\n","os.chdir(path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h-EBcGfEzaey","executionInfo":{"status":"ok","timestamp":1716999342007,"user_tz":-540,"elapsed":31599,"user":{"displayName":"h지영","userId":"09931517472368185956"}},"outputId":"58cf54ce-4115-41e3-d5cc-5d9ae9271ae3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"code","source":["!pip install timm\n","!pip install ttach"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s_NyHAFW0k9f","executionInfo":{"status":"ok","timestamp":1716999410321,"user_tz":-540,"elapsed":66926,"user":{"displayName":"h지영","userId":"09931517472368185956"}},"outputId":"62da7eca-524f-4264-b4ea-7fecb6ec408d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting timm\n","  Downloading timm-1.0.3-py3-none-any.whl (2.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.0+cu121)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->timm)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->timm)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->timm)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->timm)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->timm)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->timm)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->timm)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->timm)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->timm)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch->timm)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->timm)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->timm)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 timm-1.0.3\n","Collecting ttach\n","  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n","Installing collected packages: ttach\n","Successfully installed ttach-0.0.3\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","import torchvision\n","import cv2\n","import sys\n","import time\n","import random\n","import warnings\n","warnings.filterwarnings(action='ignore')\n","import numpy as np\n","import os.path as osp\n","import os\n","import os\n","import ttach as tta\n","import pandas as pd\n","\n","from collections import Counter\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import KFold\n","#from tensorboardX import SummaryWriter\n","device = torch.device('cuda')\n","\n","\n","def set_seeds(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False"],"metadata":{"id":"WxvJsphxy3RK","executionInfo":{"status":"ok","timestamp":1716999428689,"user_tz":-540,"elapsed":5603,"user":{"displayName":"h지영","userId":"09931517472368185956"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class STFTDataLoader(Dataset):\n","    def __init__(self, image, mode='train', transform = None):\n","        self.image = image\n","        self.mode = mode\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return self.image.shape[0]\n","\n","    def __getitem__(self, idx):\n","        sample_image = self.image[idx]\n","\n","        if self.mode == 'train':\n","            sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n","            if self.transform:\n","                sample_image = self.transform(sample_image)\n","            return sample_image\n","\n","        elif self.mode == 'val':\n","            sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n","            sample_image = self.transform(sample_image)\n","            return sample_image\n","\n","        elif self.mode == 'test':\n","            sample_image = np.array(sample_image, dtype=np.float32)\n","            sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n","            sample_image = self.transform(sample_image)\n","            return sample_image\n"],"metadata":{"id":"hPAp7S3Xy5--","executionInfo":{"status":"ok","timestamp":1716999447701,"user_tz":-540,"elapsed":590,"user":{"displayName":"h지영","userId":"09931517472368185956"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Define the autoencoder model\n","class ConvAutoencoder(nn.Module):\n","    def __init__(self):\n","        super(ConvAutoencoder, self).__init__()\n","        # 인코더 정의\n","        k=16\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, k, kernel_size=3, stride=2, padding=1),  # 3x128x128 -> 16x64x64\n","            nn.ReLU(),\n","            nn.Conv2d(k, 2*k, kernel_size=3, stride=2, padding=1),  # 16x64x64 -> 32x32x32\n","            nn.ReLU(),\n","            nn.Conv2d(2*k, 4*k, kernel_size=3, stride=2, padding=1),  # jyh 과제3-3\n","            nn.ReLU(),\n","            nn.Conv2d(4*k, 8*k, kernel_size=3, stride=2, padding=1),\n","            nn.ReLU()\n","        )\n","\n","        # 디코더 정의\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(8*k, 4*k, kernel_size=2, stride=2),  # jyh 과제3-3\n","            nn.ConvTranspose2d(4*k, 2*k, kernel_size=2, stride=2),\n","            nn.ConvTranspose2d(2*k, k, kernel_size=2, stride=2),  # 32x32x32 -> 16x64x64\n","            nn.ConvTranspose2d(k, 3, kernel_size=2, stride=2),  # 16x64x64 -> 3x128x128\n","            nn.Sigmoid()\n","            )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x"],"metadata":{"id":"pnxEcPr7zCZ6","executionInfo":{"status":"ok","timestamp":1716999450467,"user_tz":-540,"elapsed":1,"user":{"displayName":"h지영","userId":"09931517472368185956"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# **Predict (Ensemble)**"],"metadata":{"id":"OkFU9-O_td_G"}},{"cell_type":"code","source":["seed = 51\n","random.seed(seed)\n","np.random.seed(seed=seed)\n","torch.manual_seed(seed)\n","\n","lr = 1e-3\n","folds = 5\n","batch_size = 16\n","\n","resized_image = 512\n","crop_image = 498\n","\n","\n","transforms1 = tta.Compose(\n","    [\n","    #  tta.HorizontalFlip(),\n","    #  tta.Rotate90(angles=[0,90,90,180]),\n","    #  tta.FiveCrops(crop_image, crop_image)\n","    ]\n",")"],"metadata":{"id":"BbhQOuRrydaj","executionInfo":{"status":"ok","timestamp":1716999455789,"user_tz":-540,"elapsed":593,"user":{"displayName":"h지영","userId":"09931517472368185956"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def predict(models, loader):\n","  f_pred = []\n","  for batch in (loader):\n","    x = torch.tensor(batch[0], dtype = torch.float32, device = device)\n","\n","    for i, transformer in enumerate(transforms1):\n","      aug_x = transformer.augment_image(x)\n","\n","      for fold, model in enumerate(models):\n","        with torch.no_grad():\n","          if fold == 0:\n","            pred = model(aug_x)\n","          else:\n","            pred = pred + model(aug_x)\n","\n","      if i ==0:\n","        preds = pred/(len(models))\n","      else:\n","        preds = preds + pred/(len(models))\n","    preds = preds/(30)\n","    f_pred.extend(preds.argmax(1).detach().cpu().numpy().tolist())\n","  return f_pred"],"metadata":{"id":"6uRuozuztcxo","executionInfo":{"status":"ok","timestamp":1716999460659,"user_tz":-540,"elapsed":2,"user":{"displayName":"h지영","userId":"09931517472368185956"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{"id":"-dEioUJxySBZ","executionInfo":{"status":"error","timestamp":1717001147535,"user_tz":-540,"elapsed":9374,"user":{"displayName":"h지영","userId":"09931517472368185956"}},"colab":{"base_uri":"https://localhost:8080/","height":314},"outputId":"b952e262-2aa2-4ae2-d983-0472e72b821f"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"zero-dimensional arrays cannot be concatenated","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-7024bd762859>\u001b[0m in \u001b[0;36m<cell line: 310>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0msubmit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./summit.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-14-7024bd762859>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;31m#jyh 시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m     \u001b[0mtotal_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pred_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;31m#f_pred = predict(models, train_loader) #240\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"]}],"source":["# def train():\n","#     learning_rate = 1e-4\n","#     random_seed = 41\n","#     batch_size = 16\n","#     num_epochs = 100\n","#     set_seeds(random_seed)\n","\n","\n","#     raw_stft = np.load('./DATASET/stft_image_dataset.npy')\n","\n","#     # Data Augmentation\n","#     train_transform = transforms.Compose([\n","#         transforms.ToTensor(),\n","#         transforms.RandomHorizontalFlip(p=0.5),\n","#         transforms.RandomResizedCrop(size=(128, 128))\n","\n","#     ])\n","\n","#     val_transform = transforms.Compose([\n","#         transforms.ToTensor(),\n","#         transforms.RandomResizedCrop(size=(128, 128))\n","\n","#     ])\n","\n","#     # 학습 데이터, 검증 데이터 분리\n","#     kf = KFold(n_splits=5, shuffle=True)\n","\n","#     # dataset = STFTDataLoader(image=raw_stft, mode='train', transform=transform)\n","#     # data_loader = DataLoader(dataset, batch_size=16, shuffle=True, pin_memory=True)\n","\n","#     log_folder_name = 'stft_try2'\n","#     log_dir = osp.join(os.getcwd(), 'logs', f'{log_folder_name}')\n","\n","#     # log directory 생성\n","#     if not osp.exists(log_dir):\n","#         os.makedirs(log_dir)\n","\n","#     # Tensorboard 정의\n","#     writer = SummaryWriter(log_dir=log_dir)\n","\n","#     # 모델 콜\n","#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#     model = ConvAutoencoder()\n","#     model = model.to(device)\n","\n","#     # 손실 함수와 옵티마이저\n","#     criterion = nn.MSELoss()\n","#     # criterion = nn.BCELoss()\n","\n","#     optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","\n","\n","#     # 모델 학습\n","#     global_step = 0\n","#     num_fold = 5\n","#     total_error = []\n","#     val_error_ep = []\n","#     for fold, (train_indices, val_indices) in enumerate(kf.split(raw_stft)): # raw_stft의 shape: (1613, 128, 128, 3)\n","#         if fold == 1: break\n","\n","#         print(f'Fold {fold + 1}/{num_fold}')\n","\n","#         train_dataset = STFTDataLoader(image=raw_stft[train_indices], mode='train', transform=train_transform)\n","#         val_dataset = STFTDataLoader(image=raw_stft[val_indices], mode='val', transform=val_transform)\n","\n","#         train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n","#         val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n","\n","#         total_steps = len(train_loader)\n","#         for epoch in range(1, num_epochs+1):\n","#             # if epoch == 5: break\n","#             training_loss = 0\n","#             model.train()\n","#             for step, sample in enumerate(train_loader):\n","#                 # if step == 5: break\n","#                 optimizer.zero_grad()\n","\n","#                 sample = torch.tensor(sample, dtype=torch.float32, device=device)\n","#                 prediction = model(sample)\n","#                 train_loss = criterion(prediction, sample)\n","\n","#                 for param_group in optimizer.param_groups:\n","#                     current_lr = learning_rate * ((1 - epoch / num_epochs) ** 0.9)\n","#                     param_group['lr'] = current_lr\n","\n","#                 train_loss.backward()\n","#                 optimizer.step()\n","\n","#                 training_loss += train_loss.item()\n","\n","#                 # training_loss += train_loss.item()    # training_loss = training_loss + loss.item()\n","#                 # if (step % 10 == 0):\n","#                 #     avg_train_loss = training_loss/10\n","#                 # elif (step == len(train_loader)-1):\n","#                 #     avg_train_loss = training_loss / (step % 10)\n","\n","#                 if step % 10 == 0:\n","#                     train_image = torchvision.utils.make_grid(sample)\n","#                     writer.add_scalar(\"Training/Loss\", training_loss, epoch)\n","#                     writer.add_scalar(\"Training/Learning Rate\", learning_rate, epoch)\n","#                     writer.add_image('Training/Input_Image/', train_image, global_step=global_step)\n","\n","\n","#                     sys.stdout.write(f\"\\rEpoch: {epoch} \\t | step: {step+1}/{total_steps} \\t | Average Train Loss: {training_loss:.4f}\")\n","#                     sys.stdout.flush()\n","#                     time.sleep(0)\n","#             print()\n","\n","\n","\n","#             model.eval()\n","#             with torch.no_grad():\n","#                 avg_val_loss = 0\n","#                 errors = []\n","#                 for num, val_sample in enumerate(val_loader):\n","#                     val_sample = val_sample.to(device)\n","\n","#                     val_prediction = model(val_sample)\n","\n","#                     val_loss = criterion(val_prediction, val_sample)\n","#                     avg_val_loss += val_loss.item()\n","\n","#                     pred = torchvision.utils.make_grid(val_prediction)\n","#                     # writer.add_scalar(\"Validation/Loss\", val_loss, epoch)\n","#                     writer.add_image(\"Validation/Reconstructed_Image\", pred, epoch)\n","\n","#                     # Mean Absolute Error (MAE)\n","#                     error = torch.mean(torch.abs(val_prediction - val_sample), axis=0)\n","#                     error = torch.mean(error)\n","#                     errors.append(error.cpu().numpy())\n","\n","#                 errors = np.mean(np.array(errors))\n","#                 total_error.append(errors)\n","\n","#                 avg_val_loss = avg_val_loss / len(val_loader)\n","#                 writer.add_scalar(\"Validation/Loss\", avg_val_loss, epoch)\n","\n","#             print()\n","\n","#             val_error_ep.append(error)\n","\n","#             global_step += 1\n","#             if len(val_error_ep) > 1 and (val_error_ep[-1] < min(val_error_ep[:-1])):\n","#                 sys.stdout.write(f\"\\rValidation Result: Average Val Loss: {avg_val_loss:.4f}\")\n","#                 sys.stdout.flush()\n","\n","#                 torch.save({'model_state_dict': model.state_dict(),\n","#                             'optimizer_state_dict': optimizer.state_dict()},\n","#                             f'{log_dir}/CAE_checkpoint_{epoch}.pth')\n","\n","#     print('Finished Training')\n","\n","\n","\n","def get_pred_label(model_pred, t):\n","    # (0:정상, 1:불량)로 Label 변환\n","    model_pred = np.where(model_pred <= t, 0, model_pred)\n","    model_pred = np.where(model_pred > t, 1, model_pred)\n","    return model_pred\n","\n","\n","\n","def test():\n","    import matplotlib.pyplot as plt\n","    from sklearn.manifold import TSNE\n","\n","    pth = 'fold0-CAE_ckpt_100.pth'\n","    test_folder = 'stft_try4'\n","\n","    raw_stft = np.load('./stft_image_dataset.npy')\n","    test_stft = np.load('./stft_image_test_dataset_v2.npy')\n","\n","\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Resize((128, 128))\n","    ])\n","    test_dataset = STFTDataLoader(test_stft, mode='test', transform=transform)\n","    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False) #1514\n","\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    #checkpoint = torch.load(osp.join(os.getcwd(), 'logs', test_folder, pth), map_location=device)\n","\n","    model = ConvAutoencoder()\n","\n","    #jyh (4달-1회 앙상블)model.load_state_dict(checkpoint['model_state_dict'])\n","    models = []\n","    for i in range(5):\n","        model.load_state_dict(torch.load(\"./logs/stft_try4/fold\"+str(i+1)+\"/CAE_checkpoint_100.pth\"), strict=False)\n","\n","    #jyh err2 model.to('cuda')\n","    model = model.to(device)\n","    if model:\n","        models.append(model)\n","    #jyh (4달-1회 앙상블 끝)\n","    model.eval()\n","\n","    test_scores = []\n","    # latent_vectors = []\n","    for num, test_image in enumerate(test_dataloader):\n","        test_image = torch.tensor(test_image, dtype=torch.float32, device=device)\n","\n","        test_pred = model(test_image) # model prediction\n","        # z = model.encoder(test_image)\n","\n","        # latent_vectors.append(z.cpu().detach().numpy())\n","\n","        # MAE\n","        # error = torch.abs(test_image - test_pred)\n","        # score = torch.mean(torch.mean(error.squeeze(), axis=0))\n","        # mean_scores.append(score.detach().cpu().item())\n","        error = torch.mean(torch.abs(test_pred - test_image), axis=(1, 2, 3))\n","        # error = torch.mean(error)\n","        # train_score.append(error.cpu().item())\n","        test_scores.append(error.detach().cpu().numpy())\n","\n","    test_scores = np.concatenate(test_scores, axis=0)\n","    # latent_vectors = np.concatenate(latent_vectors, axis=0)\n","    # # 잠재 벡터를 2차원으로 축소하여 시각화\n","    # # tsne = TSNE(n_components=2, perplexity=10, random_state=0)\n","    # tsne = TSNE(n_components=2, random_state=0)\n","    # latent_tsne = tsne.fit_transform(latent_vectors.reshape(1510, -1))\n","\n","    # # 시각화\n","    # plt.figure(figsize=(10, 8))\n","    # plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], cmap='viridis')\n","    # plt.colorbar()\n","    # plt.xlabel('TSNE Dimension 1')\n","    # plt.ylabel('TSNE Dimension 2')\n","    # plt.title('Latent Space Visualization')\n","    # plt.savefig('./test_tsne.png')\n","\n","    train_transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.RandomResizedCrop(size=(128, 128))\n","\n","    ])\n","    # kf = KFold(n_splits=5, shuffle=True)\n","    # for fold, (train_indices, val_indices) in enumerate(kf.split(raw_stft)): # raw_stft의 shape: (1613, 128, 128, 3)\n","    #     if fold == 1: break\n","\n","    train_dataset = STFTDataLoader(image=raw_stft, mode='train', transform=train_transform)\n","    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, pin_memory=True)\n","\n","    train_score = []\n","    train_latent_vectors = []\n","    total_score = []\n","    for step, sample in enumerate(train_loader):\n","        sample = torch.tensor(sample, dtype=torch.float32, device=device)\n","        prediction = model(sample)\n","\n","        # z = model.encoder(sample)\n","        # train_latent_vectors.append(z.cpu().detach().numpy())\n","\n","        # Mean Absolute Error (MAE)\n","        error = torch.mean(torch.abs(prediction - sample), axis=(1, 2, 3))\n","        # error = torch.mean(error)\n","        # train_score.append(error.cpu().item())\n","        # error = torch.mean(error)\n","        train_score.append(error.detach().cpu().numpy())\n","        # train_score.append(error.detach().cpu().item())\n","\n","        #jyh\n","        mean_scores_per_image=[]\n","        for model in models:\n","            with torch.no_grad():\n","                test_pred = model(sample)\n","                diff = sample -test_pred\n","                error = torch.mean(torch.abs(diff), axis=(1,2,3))\n","            mean_scores_per_image.append(error.detach().cpu().numpy())\n","        score = np.mean(mean_scores_per_image)\n","        total_score.append(score)\n","\n","    # train_latent_vectors = np.concatenate(train_latent_vectors, axis=0)\n","    # # 잠재 벡터를 2차원으로 축소하여 시각화\n","    # # tsne = TSNE(n_components=2, perplexity=10, random_state=0)\n","    # tsne = TSNE(n_components=2, random_state=0)\n","    # latent_tsne = tsne.fit_transform(train_latent_vectors.reshape(1277, -1))\n","\n","    # # 시각화\n","    # plt.figure(figsize=(10, 8))\n","    # plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], cmap='viridis')\n","    # plt.colorbar()\n","    # plt.xlabel('TSNE Dimension 1')\n","    # plt.ylabel('TSNE Dimension 2')\n","    # plt.title('Latent Space Visualization')\n","    # # plt.show()\n","    # plt.savefig('./train_tsne.png')\n","\n","    train_score = np.concatenate(train_score, axis=0)\n","    train_score = np.array(train_score)\n","    t = max(train_score)\n","    # result = get_pred_label(model_pred=test_scores, t=t)\n","    # print(Counter(result))\n","\n","    #jyh 시작\n","    total_score = np.concatenate(total_score, axis=0)\n","    result = get_pred_label(model_pred=test_scores, t=t)\n","    #f_pred = predict(models, train_loader) #240\n","    #jyh 끝\n","\n","    submit = pd.read_csv('./sample_submission.csv') #1514\n","    submit['LABEL'] = result\n","    print(submit.head())\n","    submit.to_csv('./summit.csv', index=False)\n","\n","test()"]},{"cell_type":"markdown","source":["No CUDA GPUs are available   <--  model.to('cuda')\n","\n","에러2: split 2일때\n","ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (8,) + inhomogeneous part.\n"],"metadata":{"id":"drjANDRvEelq"}}]}